# this file will be sourced at start of run.sh
#
# glossary:
# master - 1 machine which coordinate slaves work, if any, h2o doesn't have master
# slaves - N machines that execute the job
# cluster - master and slaves
# client - machine where you call run.sh, same as master
#
# requirements:
# passwordless ssh from client to cluster
# run one-time setup/setup-*.sh to bootstrap apps to cluster
# populate data and provide data.csv dictionary
# ~/tmp dir must exists on every machine
# impala cluster must be aready running, and $IMPALA_MASTER as host to any impala daemon
# spark and h2o clusters will be started during the script

## hostname
#export MASTER="mr-0xd6"
#export SLAVES="mr-0xd1 mr-0xd2 mr-0xd3 mr-0xd4 mr-0xd5 mr-0xd7 mr-0xd8 mr-0xd9 mr-0xd10"
#export CLUSTER="$MASTER $SLAVES"

# task
export RUN_TASKS="groupby sort" # join

# source data
# use data.csv

# output
export CSV_TIME_FILE="time.csv"

# print csv entries
export CSV_VERBOSE=false

## spark config
#export SPARK_PRINT_LAUNCH_COMMAND=1
#export SPARK_MASTER_IP=$MASTER
#export SPARK_MASTER_PORT="17077"
#export SPARK_WORKER_IP=$SLAVES
#export SPARK_MASTER="spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT"
#export SPARK_HOME="$HOME/spark-2.0.1-bin-hadoop2.6" # recent stable
#export SPARK_HOME="$HOME/spark-2.1.0-SNAPSHOT-bin-hadoop2.6" # devel snapshot
#export SPARK_LOG_DIR=~/tmp/spark/logs # not /tmp as that's limited to 50GB and got full with 1e9 test: 'No space left on device'
#export SPARK_WORKER_DIR=~/tmp/spark/work
#export SPARK_LOCAL_DIRS=~/tmp/spark/work

## impala
#export IMPALA_HOME="$HOME/impala"
#export IMPALA_MASTER="mr-0xd2-precise1" # can be any as there is no master node in impala

## h2o
#export MEM="-Xmx200G -Xms200G"
#export H2O_HOST=$MASTER # can be any as there is no master node in h2o
#export H2O_PORT="55888"
#export H2O_NAME="${USER}_H2O"

## presto
#export PRESTO_HOME="$HOME/presto-server-0.150"
#export PRESTO_PORT=18880 # also set in $PRESTO_HOME/etc/
#export PRESTO_CLI="$HOME/presto.jar"
